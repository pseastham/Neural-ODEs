{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODEs\n",
    "\n",
    "This notebook shows how to use Julia's DiffEqFlux library to implement neural ODEs. This is just a simple example, but see [here](https://diffeqflux.sciml.ai/stable/) for official tutorials and examples.\n",
    "\n",
    "Use 'Shift + Enter' to run a cell.\n",
    "\n",
    "We first need to load in the Julia libraries for building and training neural ODEs. This step might take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, OrdinaryDiffEq, Flux, Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is load in some data. We will choose a spiral of the form $\\langle x(t),y(t) \\rangle= \\langle (1+t)\\cos(t), (1+t)\\sin(t) \\rangle$. This is inspired by an example given in the [2018 Neural ODEs paper](https://arxiv.org/abs/1806.07366)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 0.3                 # scale of noise\n",
    "t_final = 4.0           # final time point\n",
    "n_data = 60             # number of data points\n",
    "\n",
    "t_data = range(0,stop=t_final,length=n_data)\n",
    "data = zeros(2,n_data)\n",
    "data[1,:] = (t_data .+ 1).*cos.(t_data) + ϵ*rand(n_data)\n",
    "data[2,:] = (t_data .+ 1).*sin.(t_data) + ϵ*rand(n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give us an idea of what we're looking at, let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data[1,:],data[2,:],xlabel=\"x\",ylabel=\"y\",title=\"Spiral Data\",label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to set up a machine learning model. We could use a regression technique, or a classical neural network. Instead, we'll use a \"Neural ODE\", where the goal is to model the *derivatives* of our data. We could do this from scratch, but instead we will make use of the Julia library [DiffEqFlux](https://diffeqflux.sciml.ai/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = Float32[data[1,1]; data[2,1]]            # initial condition for ODE\n",
    "tspan = (0.0f0, Float32(t_final))             # span of data (as 32-bit float)\n",
    "n_dims = length(u0)                           # dimensions of data\n",
    "width = 20                                    # width of neural network\n",
    "\n",
    "model = FastChain(FastDense(n_dims, width, tanh),\n",
    "                  FastDense(width, n_dims))\n",
    "prob_neuralode = NeuralODE(model, tspan, Tsit5(), \n",
    "                           saveat = t_data,\n",
    "                           relerr = 1e-6, abserr = 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable 'model' sets up our neural network, and 'prob_neuralode' sets up the problem to be solved using the [DifferentialEquations](https://diffeq.sciml.ai/v2.0/) library. This library has many advanced solver features such as adaptive time stepping, callback control, and many, many choices for discretization. Above we chose the Tsit5 solver which is [recommended](https://diffeq.sciml.ai/stable/solvers/ode_solve/) for non-stiff systems.\n",
    "\n",
    "We now have to define two things:\n",
    "* How to evaluate this neural network\n",
    "* How the loss function is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict_neuralode(p)\n",
    "  Array(prob_neuralode(u0, p))\n",
    "end\n",
    "\n",
    "function loss_neuralode(p)\n",
    "    pred = predict_neuralode(p)\n",
    "    loss = sum(abs2, data .- pred)\n",
    "    return loss, pred\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are learning the dynamics of our data, and not the data values itself, whenever we evaluate our neural network we really need to solve an ODE system. This evaluation is computed in 'predict_neuralode', which takes in 'p' for the network parameters. In a real problem we might also want to make 'predict_neuralode' a function of the initial condition u0, but for our purposes this works fine as it is.\n",
    "\n",
    "Our loss function is a standard sum of squares, defined in 'loss_neuralode', which also depends on the network parameters.\n",
    "\n",
    "One last thing we will define is a 'callback function' to be inserted into the ODE solver. Typically these are used for event handling, but we will use it to plot the model prediction as it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "callback = function (p, l, pred; doplot = true)\n",
    "  global iter\n",
    "  iter += 1\n",
    "\n",
    "  if doplot && (mod(iter,3) == 0)\n",
    "    IJulia.clear_output(true) #Passing true says to wait until new ouput before clearing, this prevents flickering\n",
    "    plt = scatter(data[1,1:size(pred,2)], data[2,1:size(pred,2)], label = \"data\",title = string(\"iter: \",iter))\n",
    "    scatter!(plt, pred[1,1:size(pred,2)], pred[2,1:size(pred,2)], label = \"prediction\",xlim=(-5,2.5),ylim=(-4,4))\n",
    "    plot(plt) |> IJulia.display\n",
    "  end\n",
    "\n",
    "  return false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is set up to run, all that is left to do is actually train the model. Again, this is where the DiffEqFlux library does the heavy lifting, as we only need to call a single function: 'sciml_train'. \n",
    "\n",
    "This function takes in: \n",
    "* loss function (loss_neuralode)\n",
    "* model parameters (prob_neuralode.p)\n",
    "* optimization method ( ADAM(0.05) )\n",
    "* callback function (cb=callback)\n",
    "* maximum iterations it should run (maxiters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0       # reset iter (in case you run this multiple times)\n",
    "result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n",
    "                                          ADAM(0.05), cb = callback,\n",
    "                                          maxiters = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now our model is trained! The summary above give some idea of how the training process went. \n",
    "\n",
    "(Note: The NaN values in \"Convergence measures\" are actual values when I run this code outside of a jupyter notebook. Not sure what's going on here)\n",
    "\n",
    "So, how do you access the parameter values that define your trained model? With result_neuralode.minimizer. So, for example, we could generate our prediction with 'predict_neuralode(result_neuralode.minimizer)'.\n",
    "\n",
    "Occasionally, we will want to combine different optimization techniques on the same problem, for example using a cheap, coarse optimization method in the beginning and then switch to something that is more accurate once we get 'close' to the local minimum. The code below does exactly this, using the [L-BFGS optimization algorithm](https://en.wikipedia.org/wiki/Limited-memory_BFGS). Note that 'result_neuralode.minimizer' is passed in as the parameter value so that we start from the previous parameter values and don't start training the model from scratch again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode,\n",
    "                                           result_neuralode.minimizer,\n",
    "                                           LBFGS(),\n",
    "                                           cb = callback,\n",
    "                                           allow_f_increases = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! The Julia library DiffEqFlux allows us to handle much more complex cases than the one demonstrated here.\n",
    "\n",
    "For more information, see one of the following links:\n",
    "* [DiffEqFlux announcement](https://julialang.org/blog/2019/01/fluxdiffeq/)\n",
    "* [DiffEqFlux github page](https://github.com/SciML/DiffEqFlux.jl)\n",
    "* [SciML homepage](https://sciml.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
